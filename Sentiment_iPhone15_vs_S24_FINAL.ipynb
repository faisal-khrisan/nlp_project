{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ccd571a",
   "metadata": {},
   "source": [
    "# Sentiment Analysis: iPhone 15 vs Galaxy S24 (NLP + ML)\n",
    "\n",
    "**Goal:** Compare customer sentiment for iPhone 15 and Galaxy S24 using review text, and build multiple sentiment classifiers.\n",
    "\n",
    "**Models:**\n",
    "- Model A: TF-IDF (Unigram) + Logistic Regression\n",
    "- Model B: TF-IDF (Unigram+Bigram) + Logistic Regression\n",
    "- Model C: Word2Vec + SVM\n",
    "- Model D: FastText + SVM\n",
    "\n",
    "**Outputs:** EDA comparison charts, confusion matrices, model comparison, phone-wise evaluation, error analysis table, and demo-safe predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410bc634",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "Install required libraries and download the dataset (Kaggle API)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8a6065",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h5H5Ga6abrvz",
    "outputId": "c6430354-2732-4d63-a164-a206ccaecb09"
   },
   "outputs": [],
   "source": [
    "!pip -q install kaggle\n",
    "\n",
    "import os, json, shutil\n",
    "os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
    "shutil.copy(\"kaggle.json\", \"/root/.kaggle/kaggle.json\")\n",
    "os.chmod(\"/root/.kaggle/kaggle.json\", 600)\n",
    "\n",
    "# OPTION A (recommended dataset)\n",
    "!kaggle datasets download -d mohankrishnathalla/mobile-reviews-sentiment-and-specification -p /content --unzip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e57f9d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Au5aTP5xdDUL",
    "outputId": "8b2812d3-5eac-42b8-c3d1-93fbdfa9bbca"
   },
   "outputs": [],
   "source": [
    "!pip -q install nltk gensim scikit-learn seaborn matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acaf5b17",
   "metadata": {},
   "source": [
    "## 2. Imports\n",
    "Load all Python libraries used across the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a67b10",
   "metadata": {
    "id": "mM3METyagW71"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b4581b",
   "metadata": {},
   "source": [
    "## 3. NLTK Resources\n",
    "Download tokenizer and stopword resources needed for preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355ac535",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cJ7KMI10gYgK",
    "outputId": "716837b5-adb0-4ad3-9daa-d6cd4f0554e7"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e044bfe4",
   "metadata": {},
   "source": [
    "## 4. Load Dataset\n",
    "Load the dataset and inspect its structure (rows, columns, data types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9ead1c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tC4FHrCbgfT9",
    "outputId": "cc32a036-b88e-442a-8d55-cfe31f40c44a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir('/content')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6a655c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 741
    },
    "id": "Ed9o_VDkgt79",
    "outputId": "e45f35ee-5b5f-4d24-9627-8a3547386dc9"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/Mobile Reviews Sentiment.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e236a251",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8lncxQSAg8le",
    "outputId": "c4839f0e-19ab-444a-c079-42721d6267ea"
   },
   "outputs": [],
   "source": [
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cc09d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1lXK6Urng-S4",
    "outputId": "116ec2d2-093b-4c67-eec8-92649a9f34ee"
   },
   "outputs": [],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbf9496",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eWOJKrP2hxh2",
    "outputId": "b8e2d044-3c85-434a-a7ef-9f27a1aba2f9"
   },
   "outputs": [],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e756a2",
   "metadata": {},
   "source": [
    "## 5. Filter Target Phones (iPhone 15 & Galaxy S24)\n",
    "Keep only reviews that belong to iPhone 15 and Galaxy S24 for a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bd32ea",
   "metadata": {
    "id": "iOBAift1kLt4"
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns=['sentiment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c22a63",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "6VT4-sHJmWkV",
    "outputId": "79dacd3e-036a-4c8f-a8a4-ed44207f9a88"
   },
   "outputs": [],
   "source": [
    "iphone_keywords = ['iphone 15']\n",
    "samsung_keywords = ['galaxy s24', 's24']\n",
    "\n",
    "def filter_models(text):\n",
    "    text = str(text).lower()\n",
    "    if any(k in text for k in iphone_keywords):\n",
    "        return 'iPhone 15'\n",
    "    elif any(k in text for k in samsung_keywords):\n",
    "        return 'Galaxy S24'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "df['brand_model'] = df['model'].apply(filter_models)\n",
    "df = df[df['brand_model'] != 'Other']\n",
    "\n",
    "df['brand_model'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a284547c",
   "metadata": {},
   "source": [
    "## 6. Create Sentiment Labels (from Rating)\n",
    "Map rating to sentiment for transparent supervised learning: 1–2=Negative, 3=Neutral, 4–5=Positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aa2588",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "id": "-gT9tq59m3uj",
    "outputId": "9fee1d52-f784-4089-b550-7a9e25fdc77e"
   },
   "outputs": [],
   "source": [
    "def rating_to_sentiment(rating):\n",
    "    if rating <= 2:\n",
    "        return 'Negative'\n",
    "    elif rating == 3:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'\n",
    "\n",
    "df['sentiment_label'] = df['rating'].apply(rating_to_sentiment)\n",
    "\n",
    "df['sentiment_label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ec6914",
   "metadata": {},
   "source": [
    "## 7. Text Preprocessing\n",
    "Clean and normalize review text (lowercase, remove noise, tokenize, remove stopwords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd820ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "qgO-xbXym_Ha",
    "outputId": "d1f800d2-5a90-48d8-d073-26ad0fdf8237"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df['clean_review'] = df['review_text'].apply(clean_text)\n",
    "\n",
    "df[['review_text', 'clean_review']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9077355",
   "metadata": {},
   "source": [
    "## 8. Exploratory Data Analysis (EDA) & Comparison\n",
    "Visualize sentiment distribution overall and by phone model to support comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7327901",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "id": "BQOazyYznjfp",
    "outputId": "918bff7e-6a93-43fd-edbf-0852d429d08c"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='sentiment_label', data=df, palette='viridis')\n",
    "plt.title('Sentiment Distribution (iPhone 15 vs Galaxy S24)')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db1a30f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "EMG_WHbCpESi",
    "outputId": "fbe21e54-97e0-4d7e-a850-b1c91d5e0bf3"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.countplot(\n",
    "    data=df,\n",
    "    x='sentiment_label',\n",
    "    hue='brand_model',\n",
    "    palette='Set2'\n",
    ")\n",
    "\n",
    "plt.title('Sentiment Distribution by Phone Model (iPhone 15 vs Galaxy S24)')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.legend(title='Phone Model')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d46d88d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 651
    },
    "id": "2TbfRWB4kDpt",
    "outputId": "29753941-1a39-4bfd-bc88-6098c8296f85"
   },
   "outputs": [],
   "source": [
    "# --- Sentiment proportions by phone model ---\n",
    "sent_table = pd.crosstab(df['brand_model'], df['sentiment_label'], normalize='index') * 100\n",
    "sent_counts = pd.crosstab(df['brand_model'], df['sentiment_label'])\n",
    "\n",
    "display(sent_counts)\n",
    "display(sent_table.round(2))\n",
    "\n",
    "# --- Stacked bar chart (percent) ---\n",
    "sent_table[['Negative','Neutral','Positive']].plot(kind='bar', stacked=True, figsize=(8,5))\n",
    "plt.ylabel('Percentage of Reviews (%)')\n",
    "plt.title('Comparative Sentiment Composition: iPhone 15 vs Galaxy S24')\n",
    "plt.legend(title='Sentiment', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b317b19f",
   "metadata": {},
   "source": [
    "## 9. Train/Test Split\n",
    "Split the dataset using stratification to preserve sentiment class proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1190fcbc",
   "metadata": {
    "id": "DdjXnvAhnz-k"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['clean_review']\n",
    "y = df['sentiment_label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e109fc34",
   "metadata": {},
   "source": [
    "## 10. Model A: TF-IDF (Unigram) + Logistic Regression\n",
    "Baseline frequency-based model using unigram TF-IDF features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6be389",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rced2Ws6pWNf",
    "outputId": "e11eafe5-0e68-4c7b-ce75-8dedf292d1a2"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Model A: TF-IDF (Unigram) Vectorization\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 1)  # ✅ unigram only\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "X_train_tfidf.shape, X_test_tfidf.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36df2318",
   "metadata": {
    "id": "uaDgol-Pqbbk"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred_lr = logreg.predict(X_test_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbae56bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cSLXDQZlqhYI",
    "outputId": "32b47b22-477a-4077-f15c-d36cab51b305"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"Accuracy (Model A - TF-IDF Unigram + Logistic Regression):\", accuracy_score(y_test, y_pred_lr))\n",
    "\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea2b5ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "yNVLoSPGqtD3",
    "outputId": "1e98c2fd-c9f1-4bda-a55a-2a6af0b70a96"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(\n",
    "    cm_lr,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=logreg.classes_,\n",
    "    yticklabels=logreg.classes_\n",
    ")\n",
    "\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.title('Confusion Matrix – TF-IDF + Logistic Regression')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a40795",
   "metadata": {},
   "source": [
    "## 11. Model B: TF-IDF (Unigram + Bigram) + Logistic Regression\n",
    "Adds bigrams to capture short phrases such as negation (e.g., “not good”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f623465",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "30Pq3a1n1Nq7",
    "outputId": "27442fd3-5cda-412a-a00a-e9ca69ead0f9"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "tfidf_bigram = TfidfVectorizer(\n",
    "    max_features=8000,\n",
    "    ngram_range=(1, 2)   # ✅ unigram + bigram\n",
    ")\n",
    "\n",
    "X_train_tfidf_bi = tfidf_bigram.fit_transform(X_train)\n",
    "X_test_tfidf_bi = tfidf_bigram.transform(X_test)\n",
    "\n",
    "logreg_bi = LogisticRegression(max_iter=2000)\n",
    "logreg_bi.fit(X_train_tfidf_bi, y_train)\n",
    "\n",
    "y_pred_lr_bi = logreg_bi.predict(X_test_tfidf_bi)\n",
    "\n",
    "print(\"Accuracy (TF-IDF Uni+Bi + Logistic Regression):\", accuracy_score(y_test, y_pred_lr_bi))\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_lr_bi))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf02e06",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "P2D3EfsW1ShL",
    "outputId": "63da3cad-f10d-4736-fa70-ae86f0a80b7f"
   },
   "outputs": [],
   "source": [
    "cm_lr_bi = confusion_matrix(y_test, y_pred_lr_bi)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(\n",
    "    cm_lr_bi, annot=True, fmt='d', cmap='Purples',\n",
    "    xticklabels=logreg_bi.classes_, yticklabels=logreg_bi.classes_\n",
    ")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix – TF-IDF (Unigram+Bigram) + Logistic Regression\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998ded42",
   "metadata": {},
   "source": [
    "## 12. Error Analysis (Anomaly Investigation)\n",
    "Extract misclassified test samples to explain model weaknesses (neutral ambiguity, negation, mixed sentiment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d08af98",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9War4RhlxDz5",
    "outputId": "721e2943-dc84-473a-b5b4-ad3f2496a986"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Helper: build a neat error table for ANY model output\n",
    "def build_error_table(df, y_true, y_pred, model_name, n=12):\n",
    "    # Align indices (y_true is a Series with original df indices)\n",
    "    idx = y_true.index\n",
    "    y_pred_s = pd.Series(y_pred, index=idx, name=\"predicted\")\n",
    "\n",
    "    temp = df.loc[idx, ['brand_model', 'review_text', 'clean_review']].copy()\n",
    "    temp['actual'] = y_true\n",
    "    temp['predicted'] = y_pred_s\n",
    "    temp['is_error'] = temp['actual'] != temp['predicted']\n",
    "\n",
    "    errors = temp[temp['is_error']].copy()\n",
    "    errors['error_type'] = errors['actual'].astype(str) + \" → \" + errors['predicted'].astype(str)\n",
    "\n",
    "    print(f\"\\n===== ERROR ANALYSIS: {model_name} =====\")\n",
    "    print(\"Total test samples:\", len(temp))\n",
    "    print(\"Total errors:\", len(errors))\n",
    "    print(\"\\nTop error types:\")\n",
    "    print(errors['error_type'].value_counts().head(10))\n",
    "\n",
    "    # Pick balanced samples across error types\n",
    "    samples = (\n",
    "        errors.groupby('error_type', group_keys=False)\n",
    "              .apply(lambda g: g.sample(min(3, len(g)), random_state=42))\n",
    "              .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # If too many rows, take n\n",
    "    samples = samples.head(n)[['brand_model', 'actual', 'predicted', 'error_type', 'review_text']]\n",
    "\n",
    "    # Shorten long text for readability in report\n",
    "    samples['review_text'] = samples['review_text'].str.slice(0, 140) + np.where(samples['review_text'].str.len() > 140, \"…\", \"\")\n",
    "\n",
    "    return samples\n",
    "\n",
    "# --- Run for Model B (TF-IDF Uni+Bi + Logistic Regression) ---\n",
    "errors_B = build_error_table(df, y_test, y_pred_lr_bi, \"Model B: TF-IDF (Unigram+Bigram) + Logistic Regression\", n=12)\n",
    "errors_B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9bba5f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eb7J8SKsxSNN",
    "outputId": "09c9f2cc-f05f-46ed-a1ad-ddabb2b73945"
   },
   "outputs": [],
   "source": [
    "errors_B.to_csv(\"Table4_Misclassified_Examples_ModelB.csv\", index=False)\n",
    "print(\"Saved: Table4_Misclassified_Examples_ModelB.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5f1958",
   "metadata": {},
   "source": [
    "## 13. Model C: Word2Vec + SVM\n",
    "Embedding-based model using averaged Word2Vec vectors + SVM classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a69b4ed",
   "metadata": {
    "id": "B84NAa_lrorl"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [text.split() for text in df['clean_review']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ad0a05",
   "metadata": {
    "id": "lccEHP7RrrdT"
   },
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    epochs=20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283ec89a",
   "metadata": {
    "id": "eVWiOWJ3r5lY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def review_to_vector(review):\n",
    "    words = review.split()\n",
    "    vectors = [w2v_model.wv[w] for w in words if w in w2v_model.wv]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(100)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "X_w2v = np.array([review_to_vector(r) for r in df['clean_review']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6258d5",
   "metadata": {
    "id": "l0O9qnxNr_dz"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(\n",
    "    X_w2v,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8a7f08",
   "metadata": {
    "id": "eXA3ddmmsDcd"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm = LinearSVC()\n",
    "svm.fit(X_train_w2v, y_train_w2v)\n",
    "\n",
    "y_pred_svm = svm.predict(X_test_w2v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e152271",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M4HvP1GpsVRl",
    "outputId": "9d06f59c-5154-4f30-9ae0-be5d3dfe4991"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"Accuracy (Word2Vec + SVM):\",\n",
    "      accuracy_score(y_test_w2v, y_pred_svm))\n",
    "\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test_w2v, y_pred_svm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9125aeb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "hHh7tJwqsf2z",
    "outputId": "03529dde-430b-48c2-b71e-6423f73d61aa"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm_svm = confusion_matrix(y_test_w2v, y_pred_svm)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(\n",
    "    cm_svm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Greens',\n",
    "    xticklabels=svm.classes_,\n",
    "    yticklabels=svm.classes_\n",
    ")\n",
    "\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.title('Confusion Matrix – Word2Vec + SVM')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7776464a",
   "metadata": {},
   "source": [
    "## 14. Model D: FastText + SVM\n",
    "Embedding model with subword information to handle rare/unseen words better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3181b474",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hqFmHjpd1ayv",
    "outputId": "60552429-a104-4823-9949-264fd7f8d306"
   },
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Train FastText on your cleaned corpus\n",
    "sentences = [text.split() for text in df['clean_review']]\n",
    "\n",
    "ft_model = FastText(\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4\n",
    ")\n",
    "ft_model.build_vocab(sentences)\n",
    "ft_model.train(sentences, total_examples=len(sentences), epochs=20)\n",
    "\n",
    "def review_to_vector_fasttext(review, vector_size=100):\n",
    "    words = review.split()\n",
    "    vectors = [ft_model.wv[w] for w in words if w in ft_model.wv]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "X_ft = np.array([review_to_vector_fasttext(r, 100) for r in df['clean_review']])\n",
    "\n",
    "X_train_ft, X_test_ft, y_train_ft, y_test_ft = train_test_split(\n",
    "    X_ft, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "svm_ft = LinearSVC()\n",
    "svm_ft.fit(X_train_ft, y_train_ft)\n",
    "\n",
    "y_pred_ft = svm_ft.predict(X_test_ft)\n",
    "\n",
    "print(\"Accuracy (FastText + SVM):\", accuracy_score(y_test_ft, y_pred_ft))\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test_ft, y_pred_ft))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7342feb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "NUGWMQ4Q1ijJ",
    "outputId": "3a20fb98-9cdf-4336-e33b-2bf8f3813850"
   },
   "outputs": [],
   "source": [
    "cm_ft = confusion_matrix(y_test_ft, y_pred_ft)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(\n",
    "    cm_ft, annot=True, fmt='d', cmap='Oranges',\n",
    "    xticklabels=svm_ft.classes_, yticklabels=svm_ft.classes_\n",
    ")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix – FastText + SVM\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d63159",
   "metadata": {},
   "source": [
    "## 15. Model Comparison\n",
    "Compare models using Weighted F1-score to account for class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75403df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "id": "6dBB9rFCtSWg",
    "outputId": "fdfa5052-92bb-4a3c-abb7-b4725973f138"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_A = f1_score(y_test, y_pred_lr, average='weighted')        # Model A\n",
    "f1_B = f1_score(y_test, y_pred_lr_bi, average='weighted')     # Model B\n",
    "f1_C = f1_score(y_test_w2v, y_pred_svm, average='weighted')   # Model C\n",
    "f1_D = f1_score(y_test_ft, y_pred_ft, average='weighted')     # Model D\n",
    "\n",
    "labels = ['A: TF-IDF Uni+LR', 'B: TF-IDF Uni+Bi+LR', 'C: Word2Vec+SVM', 'D: FastText+SVM']\n",
    "scores = [f1_A, f1_B, f1_C, f1_D]\n",
    "\n",
    "plt.figure(figsize=(9,4))\n",
    "plt.bar(labels, scores)\n",
    "plt.ylabel('Weighted F1-score')\n",
    "plt.title('Model Comparison (Weighted F1)')\n",
    "plt.ylim(0,1)\n",
    "plt.xticks(rotation=20, ha='right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4fb929",
   "metadata": {},
   "source": [
    "## 16. Phone-wise Evaluation (Comparative Proof)\n",
    "Evaluate each model separately on iPhone 15 vs Galaxy S24 test reviews to strengthen the comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30782ec",
   "metadata": {
    "id": "ah_UHXwdkb2V"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "def phone_wise_evaluation(df, y_true, y_pred, title):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(title)\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    test_idx = y_true.index  # keeps original indices\n",
    "    phone_series = df.loc[test_idx, 'brand_model']\n",
    "\n",
    "    for phone in ['iPhone 15', 'Galaxy S24']:\n",
    "        mask = (phone_series == phone)\n",
    "        y_t = y_true[mask]\n",
    "        y_p = pd.Series(y_pred, index=test_idx)[mask]\n",
    "\n",
    "        print(f\"\\n--- {phone} ---\")\n",
    "        print(\"Accuracy:\", round(accuracy_score(y_t, y_p), 4))\n",
    "        print(classification_report(y_t, y_p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2d1e88",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O0ywSLALkkT-",
    "outputId": "55267184-c8b5-468f-ec9c-23c7f1125d0d"
   },
   "outputs": [],
   "source": [
    "phone_wise_evaluation(df, y_test, y_pred_lr, \"Model A: TF-IDF (Unigram) + Logistic Regression (Phone-wise Results)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c40cbe5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UrU0LX0RkpQr",
    "outputId": "49a33054-3425-4889-8a19-6b48b954f94e"
   },
   "outputs": [],
   "source": [
    "phone_wise_evaluation(df, y_test, y_pred_lr_bi, \"Model B: TF-IDF (Unigram+Bigram) + Logistic Regression (Phone-wise Results)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210386a9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X2CXLTgJksma",
    "outputId": "234af029-07a8-4476-e06c-5b078ddb622e"
   },
   "outputs": [],
   "source": [
    "phone_wise_evaluation(df, y_test_w2v, y_pred_svm, \"Model C: Word2Vec + SVM (Phone-wise Results)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3101ff89",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-sioYxB0kyHB",
    "outputId": "20e3b24c-0791-4907-9d1c-48088b076e0b"
   },
   "outputs": [],
   "source": [
    "phone_wise_evaluation(df, y_test_ft, y_pred_ft, \"Model D: FastText + SVM (Phone-wise Results)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6494f9ac",
   "metadata": {},
   "source": [
    "## 17. Demonstration (Presentation-Safe)\n",
    "Show correct labelled outputs using real test-set examples + one challenging case (to explain limitations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad0cd77",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IZuqX8NDxj6c",
    "outputId": "aafb6ad4-48c2-4f74-850a-23cdc8a42c4e"
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# DEMO-SAFE PREDICTION FUNCTIONS\n",
    "# -----------------------------\n",
    "\n",
    "def predict_model_B(text):\n",
    "    \"\"\"Model B: TF-IDF (Unigram+Bigram) + Logistic Regression\"\"\"\n",
    "    text = \"\" if text is None else str(text).strip()\n",
    "    if len(text) == 0:\n",
    "        return \"Invalid input (empty text)\"\n",
    "    cleaned = clean_text(text)\n",
    "    vec = tfidf_bigram.transform([cleaned])\n",
    "    pred = logreg_bi.predict(vec)[0]\n",
    "    proba = logreg_bi.predict_proba(vec)[0]\n",
    "    return pred, float(np.max(proba))\n",
    "\n",
    "def demo_from_testset(model_pred_fn, y_true, title, per_class=2):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(title)\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    test_df = df.loc[y_true.index, ['brand_model', 'review_text']].copy()\n",
    "    test_df['actual'] = y_true\n",
    "    test_df = test_df.drop_duplicates(subset=['review_text'])  # ✅ avoid repeated examples\n",
    "\n",
    "    # Predict on all test rows\n",
    "    preds = []\n",
    "    confs = []\n",
    "    for t in test_df['review_text']:\n",
    "        out = model_pred_fn(t)\n",
    "        if isinstance(out, tuple):\n",
    "            p, c = out\n",
    "        else:\n",
    "            p, c = out, None\n",
    "        preds.append(p)\n",
    "        confs.append(c)\n",
    "    test_df['predicted'] = preds\n",
    "    test_df['confidence'] = confs\n",
    "    test_df['correct'] = test_df['predicted'] == test_df['actual']\n",
    "\n",
    "    # Show correct examples per class (safe demo)\n",
    "    print(\"\\n--- Correct examples (safe demo) ---\")\n",
    "    shown = 0\n",
    "    for cls in ['Positive', 'Neutral', 'Negative']:\n",
    "        subset = test_df[(test_df['actual'] == cls) & (test_df['correct'])]\n",
    "        sample = subset.head(per_class)\n",
    "        if len(sample) > 0:\n",
    "            for _, row in sample.iterrows():\n",
    "                print(f\"\\nPhone: {row['brand_model']}\")\n",
    "                print(f\"Review: {row['review_text']}\")\n",
    "                print(f\"Actual: {row['actual']} | Predicted: {row['predicted']} | Confidence: {row['confidence']:.3f}\")\n",
    "                shown += 1\n",
    "\n",
    "    # Show 1 challenging misclassified example (for explanation)\n",
    "    wrong = test_df[~test_df['correct']].head(1)\n",
    "    if len(wrong) > 0:\n",
    "        row = wrong.iloc[0]\n",
    "        print(\"\\n--- Challenging example (to explain limitations) ---\")\n",
    "        print(f\"Phone: {row['brand_model']}\")\n",
    "        print(f\"Review: {row['review_text']}\")\n",
    "        print(f\"Actual: {row['actual']} | Predicted: {row['predicted']} | Confidence: {row['confidence']}\")\n",
    "        print(\"Explanation: Neutral and mixed-sentiment text can be ambiguous; models may confuse it with Positive/Negative.\")\n",
    "    else:\n",
    "        print(\"\\nNo misclassified example found in the first rows (good!).\")\n",
    "\n",
    "# Run demo using Model B (best for negation phrases)\n",
    "demo_from_testset(predict_model_B, y_test, \"DEMONSTRATION: Model B (TF-IDF Uni+Bi + Logistic Regression)\", per_class=2)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
